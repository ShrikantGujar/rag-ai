# Document Chat AI

A **Retrieval-Augmented Generation (RAG)** system that allows users to upload documents (PDF, text, etc.) and ask natural language questions to retrieve answers. Built with **Node.js backend** and **React frontend**.

---

##  Features

- Upload documents and process them into a local vector store
- Ask questions and get responses from your uploaded documents
- Modern, responsive UI built with Material-UI (MUI)
- Local vector storage (FAISS-like) for fast retrieval
- Integration with **Cohere embeddings** for document vectorization

---

##  Tech Stack

### Backend

- Node.js + Express
- Multer for file uploads
- Cohere embeddings API
- Local vector store using FAISS-style in-memory storage
- REST API endpoints

### Frontend

- React (Create React App)
- Material-UI (MUI) for components and layout
- Axios for HTTP requests

---

##  Project Structure

rag-system/
├─ backend/
│ ├─ src/
│ │ ├─ controller/ # REST controllers
│ │ ├─ service/ # Document parsing, embeddings, vector store
│ │ └─ server.js # Backend entry point
│ ├─ package.json
│ └─ .env # API keys, port
├─ frontend/
│ ├─ src/
│ │ ├─ App.js # React frontend
│ │ └─ index.js
│ ├─ package.json
│ └─ public/
└─ README.md

---

## ⚡ Quick Setup Instructions

### Backend

1. Navigate to `backend` folder:

cd backend
npm install
Create a .env file:

PORT=8000
COHERE_API_KEY=YOUR_COHERE_API_KEY
Start the backend:


npm start
Health check endpoint: http://localhost:8000/health

Upload endpoint: http://localhost:8000/api/v1/upload/file

Query endpoint: http://localhost:8000/api/v1/upload/query

Frontend
Navigate to frontend folder:


cd frontend
npm install
npm start
Runs on http://localhost:3000

Connects to backend at http://localhost:8000

Architecture Overview

[React Frontend] <--HTTP--> [Node.js Backend]
        |
        |-- Upload Document --> [Document Parser & Chunker] --> [Cohere Embeddings] --> [Local Vector Store]
        |
        |-- Ask Query --> [Vector Store Retrieval] --> [LLM Answer Generation] --> [Response to Frontend]
Backend handles document ingestion, chunking, embedding, vector storage, and query retrieval

Frontend handles file upload, query input, and displaying results

## Production Considerations
To scale and productionize:

Use a managed vector database (e.g., Pinecone, Weaviate) instead of local memory
Deploy backend as containerized microservice (Docker + Kubernetes)
Use a cloud object store (S3 / GCS / Azure Blob/ OCI Object storage) for documents
Add authentication and role-based access
Implement logging, monitoring, and observability (Grafana)
Use caching for frequent queries

## RAG / LLM Approach & Decisions
LLM / Embedding Model: Cohere embeddings for vectorization, LLM for answer generation
Vector Store: Local FAISS-like memory for prototype, scalable vector DB for production
Chunking: Text split into 500–1000 token chunks
Prompt & Context Management: Query retrieves top-K relevant chunks, injected into LLM prompt
Guardrails / Quality: Basic validation on query and file type
Observability: Console logs; production would include structured logging & monitoring

## Key Technical Decisions
Used Node.js + Express for lightweight backend API
Used Material-UI for rapid, attractive frontend development
Chose Cohere embeddings for simplicity and free API access
Local vector store for prototyping; easier than full DB integration initially
Modular services (parser, embeddings, vectorstore) for clean architecture

## Engineering Standards Followed
Modular code structure (controllers, services)
Promises / async-await for async operations
Proper error handling and logging
Clean, readable code
Frontend responsive design with reusable components

## Skipped / limited:
Authentication
Multi-user handling
Full production-grade monitoring

## How AI tools were used in development
Used AI coding assistants to help scaffold services and frontend layout
Carefully reviewed all AI-generated code, modified for project requirements
Ensured modular, maintainable, and readable code

# What I’d do differently with more time
Add real-time chat interface with streaming LLM responses
Add highlighting of relevant text in query results
Switch local vector store to cloud-managed vector DB
Add unit and integration tests for backend services

## Notes
This README is written with thoughts, not generated by an LLM
Backend must be running before frontend queries work
Cohere API key is required for embeddings and LLM responses

